D:\Users\pengge\anaconda3\envs\pytorch\lib\site-packages\torch\cuda\amp\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
Train :   0%|          | 0/1 [00:00<?, ?it/s]
Train :   0%|          | 0/1 [00:00<?, ?it/s]D:\Users\pengge\anaconda3\envs\pytorch\lib\site-packages\torch\autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
Train :   0%|          | 0/1 [00:08<?, ?it/s]
Traceback (most recent call last):
  File "D:\PythonPrograms\pytorch\pengge\U-Net-template\train.py", line 117, in <module>
    run_training(model, optimizer, scheduler, run, CFG.epochs, train_loader, valid_loader)
  File "D:\PythonPrograms\pytorch\pengge\U-Net-template\train.py", line 45, in run_training
    train_loss = train_one_epoch(model, optimizer, scheduler,
  File "D:\PythonPrograms\pytorch\pengge\U-Net-template\utils\train_one_epoch.py", line 45, in train_one_epoch
    scaler.scale(loss).backward()
  File "D:\Users\pengge\anaconda3\envs\pytorch\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "D:\Users\pengge\anaconda3\envs\pytorch\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn